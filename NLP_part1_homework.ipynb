{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Данный ноутбук использовал окружение google-colabф\n",
        "%pip install catboost fasttext -q"
      ],
      "metadata": {
        "id": "rwlxK5AYASaT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e64e5b-01f4-430a-a368-c168dab5557d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание \"NLP. Часть 1\""
      ],
      "metadata": {
        "id": "3xRUXhCVUzur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ],
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ],
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Реализовать One-Hot векторизацию текстов"
      ],
      "metadata": {
        "id": "eemkFZ1tVLw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[List[int]]:\n",
        "\n",
        "    if vocab is None or vocab_index is None:\n",
        "        raise ValueError(\"vocab/vacab_index = None\")\n",
        "\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    one_hot = []\n",
        "    for word in words:\n",
        "        vector = [0] * len(vocab)\n",
        "        if word in vocab_index:\n",
        "            idx = vocab_index[word]\n",
        "            vector[idx] = 1\n",
        "        one_hot.append(vector)\n",
        "    return one_hot\n",
        "\n",
        "def test_one_hot_vectorization(\n",
        "    vocab: List[str],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result[0]) != expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for i, word in enumerate(words_in_text):\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[i][idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_one_hot_vectorization(vocab, vocab_index)"
      ],
      "metadata": {
        "id": "Q2-LJcmbTe04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630bcf4f-2a8c-46a1-f498-80d078b12434"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot-Vectors test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Реализовать Bag-of-Words"
      ],
      "metadata": {
        "id": "hAF8IOYMVT3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "    return word_counts\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_bag_of_words_vectorization()"
      ],
      "metadata": {
        "id": "ScFuXh_9TtJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909dcbc5-5f33-4362-c669-aeaa1ee80a69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad-of-Words test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "d6LblWJfX2kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf_vectorization(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[float]:\n",
        "\n",
        "    if corpus is None or vocab is None or vocab_index is None:\n",
        "        raise ValueError(\"_None_\")\n",
        "\n",
        "    tfidf_vector = [0.0] * len(vocab)\n",
        "    words = normalize_pretokenize_text(text)\n",
        "\n",
        "    tf_counts = {}\n",
        "    for word in words:\n",
        "        if word in tf_counts:\n",
        "            tf_counts[word] += 1\n",
        "        else:\n",
        "            tf_counts[word] = 1\n",
        "\n",
        "    doc_freq = {}\n",
        "    for word_in_vocab in vocab:\n",
        "        doc_freq[word_in_vocab] = 0\n",
        "\n",
        "    num_documents = len(corpus)\n",
        "    for one_document_from_corpus in corpus:\n",
        "        unique_words_in_doc = set(normalize_pretokenize_text(one_document_from_corpus))\n",
        "        for word in unique_words_in_doc:\n",
        "            if word in doc_freq:\n",
        "                doc_freq[word] += 1\n",
        "\n",
        "    idf_scores = {}\n",
        "    for word_in_vocab in vocab:\n",
        "        df_t = doc_freq[word_in_vocab]\n",
        "        idf_scores[word_in_vocab] = math.log(num_documents / (df_t + 1))\n",
        "\n",
        "    for word_from_our_text, tf_value in tf_counts.items():\n",
        "        if word_from_our_text in vocab_index:\n",
        "            word_idx = vocab_index[word_from_our_text]\n",
        "            idf_value = idf_scores[word_from_our_text]\n",
        "            tfidf_vector[word_idx] = tf_value * idf_value\n",
        "\n",
        "    return tfidf_vector\n",
        "\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown\"\n",
        "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"TF-IDF test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "GKIyS724T0XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4b0645-0926-44a4-ef60-d85aeacf74cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4 (1 балл)\n",
        "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
        "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
        "где $N(word, context)$ -- число вхождений слова $word$ в окно $context$ (размер окна -- гиперпараметр)"
      ],
      "metadata": {
        "id": "T0f9FZCrX5_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppmi_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> List[float]:\n",
        "\n",
        "    if corpus is None or vocab is None or vocab_index is None:\n",
        "        raise ValueError(\"corpus, vocab, and vocab_index must be provided\")\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "    ppmi_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "    word_freq = Counter() # N(word) - сколько раз слово встречается в корпусе\n",
        "    pair_freq = Counter() # N(word, context) - сколько раз пара встречается в окне\n",
        "    total_pairs = 0       # |(word, context)| - общее количество всех пар в окнах\n",
        "\n",
        "    for doc in corpus:\n",
        "        words = normalize_pretokenize_text(doc)\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            word_freq[word] += 1 # частота каждого слова\n",
        "\n",
        "            start_window = max(0, i - window_size)\n",
        "            end_window = min(len(words), i + window_size + 1)\n",
        "\n",
        "            for j in range(start_window, end_window):\n",
        "                if i != j:\n",
        "                    context_word = words[j]\n",
        "                    pair_freq[(word, context_word)] += 1\n",
        "                    total_pairs += 1\n",
        "\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    for target_word_str in vocab:\n",
        "        target_idx = vocab_index[target_word_str]\n",
        "        N_word = word_freq[target_word_str]\n",
        "\n",
        "        for context_word_str in vocab:\n",
        "            context_idx = vocab_index[context_word_str]\n",
        "            N_context = word_freq[context_word_str]\n",
        "\n",
        "            N_word_context = pair_freq[(target_word_str, context_word_str)]\n",
        "\n",
        "            pmi = 0.0\n",
        "            if N_word_context > 0 and N_word > 0 and N_context > 0 and total_pairs > 0:\n",
        "                numerator = N_word_context * total_pairs\n",
        "                denominator = N_word * N_context\n",
        "\n",
        "                if denominator > epsilon:\n",
        "                    pmi = math.log(numerator / denominator)\n",
        "\n",
        "            ppmi_matrix[target_idx, context_idx] = max(0.0, pmi)\n",
        "\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    text_ppmi_vector = np.zeros(vocab_size, dtype=np.float32)\n",
        "    word_count_in_input_text_in_vocab = 0\n",
        "\n",
        "    for word in words:\n",
        "        if word in vocab_index:\n",
        "            word_idx = vocab_index[word]\n",
        "            text_ppmi_vector += ppmi_matrix[word_idx, :]\n",
        "            word_count_in_input_text_in_vocab += 1\n",
        "\n",
        "    if word_count_in_input_text_in_vocab > 0:\n",
        "        text_ppmi_vector /= word_count_in_input_text_in_vocab\n",
        "\n",
        "    return text_ppmi_vector.tolist()\n",
        "\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "HgHmNZy75XFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239d22e8-f711-46b8-8f20-7ad1aaef37f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5 (1 балл)\n",
        "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
      ],
      "metadata": {
        "id": "FK29va3PBH_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fasttext_embeddings(text: str, model: any) -> List[np.ndarray]:\n",
        "\n",
        "    if model is None:\n",
        "        raise ValueError(\"Передай модель в фукнцию\")\n",
        "\n",
        "    words = normalize_pretokenize_text(text)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for word in words:\n",
        "        word_vector = model.get_word_vector(word)\n",
        "        embeddings.append(word_vector)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "tOe8dRLl5eqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    bert_tokenizer: BertTokenizer,\n",
        "    bert_model: BertModel\n",
        ") -> np.ndarray:\n",
        "\n",
        "    if bert_tokenizer is None or bert_model is None:\n",
        "        raise ValueError(\"Передай модель в фукнцию и токенизатор\")\n",
        "\n",
        "    inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return cls_embedding.cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "A9GXy6n0AtsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7dea7d1"
      },
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "model_fast = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6 (1.5 балла)\n",
        "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ],
      "metadata": {
        "id": "E_KoKolrD49R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_dataset(\n",
        "    dataset_name: str = \"imdb\",\n",
        "    vectorizer_type: str = \"bow\",\n",
        "    split: str = \"train\",\n",
        "    sample_size: int = 2500,\n",
        "    ft_model: Any = None,  # модели FastText\n",
        "    bert_tokenizer: Any = None, # для BERT токенизатора\n",
        "    bert_model: Any = None      # для BERT модели\n",
        ") -> Tuple[List[str], List[Any], List[int]]:\n",
        "\n",
        "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
        "\n",
        "    if sample_size:\n",
        "        dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
        "\n",
        "    texts = [item['text'] for item in dataset if 'text' in item and item['text'].strip()]\n",
        "    labels = [item['label'] for item in dataset if 'label' in item]\n",
        "\n",
        "    def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "        all_words = []\n",
        "        for text in texts:\n",
        "            words = normalize_pretokenize_text(text)\n",
        "            all_words.extend(words)\n",
        "        vocab = sorted(set(all_words))\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "        return vocab, vocab_index\n",
        "\n",
        "    vocab, vocab_index = build_vocab(texts)\n",
        "\n",
        "    vectorized_data = []\n",
        "    for text in texts:\n",
        "        if vectorizer_type == \"one_hot\":\n",
        "            vectorized_data.append(one_hot_vectorization(text, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"bow\":\n",
        "            bow_dict = bag_of_words_vectorization(text)\n",
        "            vector = [bow_dict.get(word, 0) for word in vocab]\n",
        "            vectorized_data.append(vector)\n",
        "        elif vectorizer_type == \"tfidf\":\n",
        "            vectorized_data.append(tf_idf_vectorization(text, texts, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"ppmi\":\n",
        "            vectorized_data.append(ppmi_vectorization(text, texts, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"fasttext\":\n",
        "            embeddings = get_fasttext_embeddings(text, ft_model)\n",
        "            if embeddings:\n",
        "                avg_embedding = np.mean(embeddings, axis=0)\n",
        "                vectorized_data.append(avg_embedding.tolist())\n",
        "            else:\n",
        "                vectorized_data.append([0] * 300)\n",
        "        elif vectorizer_type == \"bert\":\n",
        "            embedding = get_bert_embeddings(text, bert_tokenizer, bert_model)\n",
        "            vectorized_data.append(embedding.tolist())\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
        "\n",
        "    return vocab, vectorized_data, labels"
      ],
      "metadata": {
        "id": "zsc98L8JE8G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "def train(\n",
        "    embeddings_method=\"bow\",\n",
        "    test_size=0.2,\n",
        "    val_size=0.2,\n",
        "    cv_folds=5\n",
        "):\n",
        "    vocab, X, y = vectorize_dataset(\"imdb\", embeddings_method, \"train\")\n",
        "    _, X_test, y_test = vectorize_dataset(\"imdb\", embeddings_method, \"test\")\n",
        "\n",
        "    # Your code here"
      ],
      "metadata": {
        "id": "DRRw01XiBg6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for embeddings_method in [\"bow\", \"one_hot\", \"tfidf\", \"ppmi\", \"fasttext\", \"bert\"]:\n",
        "    train(embeddings_method=embeddings_method)"
      ],
      "metadata": {
        "id": "naMqAkjqFHAe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}